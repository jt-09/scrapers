{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b717ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (4.33.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: textblob in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions~=4.13.2 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from pandas) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from undetected-chromedriver) (2.32.4)\n",
      "Requirement already satisfied: websockets in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: click in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jayth\\onedrive\\documents\\scrapers\\venv\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\jayth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jayth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jayth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\jayth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\jayth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\jayth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4 textblob pandas matplotlib undetected-chromedriver setuptools\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9557c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f638c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_reviews(asin, pages=1):\n",
    "    \"\"\"\n",
    "    Scrapes Amazon product reviews using standard Selenium with stealth settings.\n",
    "\n",
    "    Parameters:\n",
    "        asin (str): The ASIN of the Amazon product.\n",
    "        pages (int): Number of review pages to scrape.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Contains review rating, title, and body text.\n",
    "    \"\"\"\n",
    "    print(f\"Collecting reviews for ASIN: {asin}\")\n",
    "\n",
    "    # Configure Chrome browser options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run without opening a window\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    # Start WebDriver (make sure chromedriver is installed and in PATH)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    all_reviews = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"https://www.amazon.com/product-reviews/{asin}/?pageNumber={page}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(3 + page)  # Add a delay to reduce chance of blocking\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Detect login redirection (a sign of bot detection)\n",
    "        title = soup.title.string if soup.title else \"\"\n",
    "        if \"Sign-In\" in title:\n",
    "            print(\"Blocked or redirected to login. Stopping scrape.\")\n",
    "            driver.quit()\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        reviews = soup.select('div[data-hook=\"review\"]')\n",
    "\n",
    "        if not reviews:\n",
    "            print(f\"No reviews found on page {page}\")\n",
    "            continue\n",
    "\n",
    "        for r in reviews:\n",
    "            try:\n",
    "                rating = r.select_one('i[data-hook=\"review-star-rating\"]').text.split()[0]\n",
    "                title = r.select_one('a[data-hook=\"review-title\"]').text.strip()\n",
    "                body = r.select_one('span[data-hook=\"review-body\"]').text.strip()\n",
    "                all_reviews.append({\n",
    "                    \"rating\": float(rating),\n",
    "                    \"title\": title,\n",
    "                    \"body\": body\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"Page {page} scraped\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    if not all_reviews:\n",
    "        print(\"No reviews were extracted.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e0ba3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(df):\n",
    "    \"\"\"\n",
    "    Applies sentiment analysis to each review using TextBlob.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the review 'body' text\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The same DataFrame with added polarity and sentiment columns\n",
    "    \"\"\"\n",
    "    print(\"analysing sentment...\")\n",
    "    \n",
    "    # Calculate polarity score using TextBlob\n",
    "    df['polarity'] = df['body'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Categorize sentiment based on polarity score\n",
    "    df['sentiment'] = df['polarity'].apply(\n",
    "        lambda p: 'positive' if p > 0 else ('negative' if p < 0 else 'neutral')\n",
    "    )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30cdb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment(df):\n",
    "    \"\"\"\n",
    "    Plots a bar chart showing the count of positive, neutral, and negative reviews.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame with a 'sentiment' column\n",
    "    \"\"\"\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "    # Bar chart with custom colors for clarity\n",
    "    sentiment_counts.plot(kind='bar', title='Sentiment Distribution', color=['green', 'gray', 'red'])\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Review Count')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59419268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reviews for ASIN: B0B92Y18GT\n",
      "No reviews found on page 1\n",
      "No reviews found on page 2\n",
      "No reviews were extracted.\n",
      "analysing sentment...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m df_reviews.head()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Analyze sentiment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df_analyzed = \u001b[43manalyze_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#  Save the result to a CSV file\u001b[39;00m\n\u001b[32m     15\u001b[39m df_analyzed.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mamazon_reviews_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masin\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36manalyze_sentiment\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33manalysing sentment...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Calculate polarity score using TextBlob\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mpolarity\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbody\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x).sentiment.polarity)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Categorize sentiment based on polarity score\u001b[39;00m\n\u001b[32m     17\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mpolarity\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[33m'\u001b[39m\u001b[33mpositive\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mnegative\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mneutral\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayth\\OneDrive\\Documents\\scrapers\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayth\\OneDrive\\Documents\\scrapers\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'body'"
     ]
    }
   ],
   "source": [
    "# target ASIN and number of pages to scrape\n",
    "asin = \"B0B92Y18GT\"\n",
    "pages = 2\n",
    "\n",
    "# Scrape reviews\n",
    "df_reviews = scrape_reviews(asin, pages)\n",
    "\n",
    "# Preview first few reviews\n",
    "df_reviews.head()\n",
    "\n",
    "# Analyze sentiment\n",
    "df_analyzed = analyze_sentiment(df_reviews)\n",
    "\n",
    "#  Save the result to a CSV file\n",
    "df_analyzed.to_csv(f\"amazon_reviews_{asin}.csv\", index=False)\n",
    "\n",
    "# Show a preview\n",
    "df_analyzed.head()\n",
    "\n",
    "# ðŸ“Š Plot sentiment distribution\n",
    "plot_sentiment(df_analyzed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
